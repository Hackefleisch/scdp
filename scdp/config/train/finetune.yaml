# reproducibility
seed: 0
deterministic: False

# PyTorch Lightning Trainer https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html
trainer:
  fast_dev_run: False
  accelerator: 'gpu'
  strategy: ddp
  devices: 4
  precision: 32
  max_epochs: 200
  max_steps: 300000
  num_sanity_val_steps: 2
  gradient_clip_val: 0.5
  val_check_interval: 1.
  deterministic: ${train.deterministic}

optim:
  _target_: torch.optim.Adam
  lr: 2e-5
  betas: [0.9, 0.999]
  eps: 1e-08
  weight_decay: 0.0

lr_scheduler:
  _target_: scdp.model.utils.PowerDecayScheduler
  warmup_steps: 1000
  alpha: 0.96
  beta: 5319 # decay to 2e-6 in 300k steps
lr_schedule_freq: 1

ema:
  decay: 0.995

restore:
  ckpt_or_run_path: null
  mode: null # nuall, finetune, hotstart, continue

monitor:
  metric: 'loss/val'
  mode: 'min'

callbacks:
  - _target_: lightning.pytorch.callbacks.EarlyStopping
    patience: 20
    verbose: False
    monitor: ${train.monitor.metric}
    mode: ${train.monitor.mode}

  - _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: ${core.storage_dir}
    save_top_k: 2
    save_last: True
    verbose: False
    monitor: ${train.monitor.metric}
    mode: ${train.monitor.mode}

  - _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: "step"
    log_momentum: False

  - _target_: lightning.pytorch.callbacks.progress.tqdm_progress.TQDMProgressBar
    refresh_rate: 20

logging:
  upload:
    run_files: true
    source: true

  wandb:
    name: ${core.expname}
    project: ${core.project_name}
    entity: null
    log_model: ${..upload.run_files}
    mode: 'online'
    tags: ${core.tags}
 
  wandb_watch:
    log: 'all'
    log_freq: 100
